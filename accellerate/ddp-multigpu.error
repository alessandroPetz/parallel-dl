[W318 14:32:32.949739283 socket.cpp:464] [c10d] waitForInput: poll for socket SocketImpl(fd=19, addr=[gn02]:60274, remote=[gn01]:29500) returned 0, likely a timeout
[W318 14:32:32.949888471 socket.cpp:489] [c10d] waitForInput: socket SocketImpl(fd=19, addr=[gn02]:60274, remote=[gn01]:29500) timed out after 900000ms
[W318 14:32:32.649384647 socket.cpp:464] [c10d] waitForInput: poll for socket SocketImpl(fd=19, addr=[gn01]:47860, remote=[gn01]:29500) returned 0, likely a timeout
[W318 14:32:32.649495996 socket.cpp:489] [c10d] waitForInput: socket SocketImpl(fd=19, addr=[gn01]:47860, remote=[gn01]:29500) timed out after 900000ms
Traceback (most recent call last):
  File "/home/apetrella/miniconda3/envs/llm/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1190, in launch_command
    multi_gpu_launcher(args)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/accelerate/commands/launch.py", line 808, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/run.py", line 909, in run
Traceback (most recent call last):
  File "/home/apetrella/miniconda3/envs/llm/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    elastic_launch(
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    args.func(args)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1190, in launch_command
    result = f(*args, **kwargs)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 711, in run
    result = self._invoke_run(role)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 864, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 683, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 513, in _rendezvous
    workers = self._assign_worker_ranks(
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 605, in _assign_worker_ranks
    role_infos_bytes = store.multi_get(
torch.distributed.DistStoreError: wait timeout after 900000ms, keys: /none/torchelastic/role_info/0, /none/torchelastic/role_info/1
    multi_gpu_launcher(args)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/accelerate/commands/launch.py", line 808, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 711, in run
    result = self._invoke_run(role)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 864, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 683, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 513, in _rendezvous
    workers = self._assign_worker_ranks(
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 605, in _assign_worker_ranks
    role_infos_bytes = store.multi_get(
torch.distributed.DistStoreError: wait timeout after 900000ms, keys: /none/torchelastic/role_info/0, /none/torchelastic/role_info/1
srun: error: gn01: task 0: Exited with exit code 1
srun: error: gn02: task 1: Exited with exit code 1
