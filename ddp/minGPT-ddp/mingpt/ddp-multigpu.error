W0214 12:10:43.100671 2580877 site-packages/torch/distributed/run.py:792] 
W0214 12:10:43.100671 2580877 site-packages/torch/distributed/run.py:792] *****************************************
W0214 12:10:43.100671 2580877 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0214 12:10:43.100671 2580877 site-packages/torch/distributed/run.py:792] *****************************************
/home/apetrella/Workspace/parallel-dl/ddp/minGPT-ddp/mingpt/trainer.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
/home/apetrella/Workspace/parallel-dl/ddp/minGPT-ddp/mingpt/trainer.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
W0214 12:11:18.139874 2581056 site-packages/torch/distributed/run.py:792] 
W0214 12:11:18.139874 2581056 site-packages/torch/distributed/run.py:792] *****************************************
W0214 12:11:18.139874 2581056 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0214 12:11:18.139874 2581056 site-packages/torch/distributed/run.py:792] *****************************************
W0214 12:11:19.149877 532055 site-packages/torch/distributed/run.py:792] 
W0214 12:11:19.149877 532055 site-packages/torch/distributed/run.py:792] *****************************************
W0214 12:11:19.149877 532055 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0214 12:11:19.149877 532055 site-packages/torch/distributed/run.py:792] *****************************************
/home/apetrella/Workspace/parallel-dl/ddp/minGPT-ddp/mingpt/trainer.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
/home/apetrella/Workspace/parallel-dl/ddp/minGPT-ddp/mingpt/trainer.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
/home/apetrella/Workspace/parallel-dl/ddp/minGPT-ddp/mingpt/trainer.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
/home/apetrella/Workspace/parallel-dl/ddp/minGPT-ddp/mingpt/trainer.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
