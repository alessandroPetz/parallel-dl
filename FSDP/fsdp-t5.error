W0220 14:05:23.846762 1251304 site-packages/torch/distributed/run.py:792] 
W0220 14:05:23.846762 1251304 site-packages/torch/distributed/run.py:792] *****************************************
W0220 14:05:23.846762 1251304 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 14:05:23.846762 1251304 site-packages/torch/distributed/run.py:792] *****************************************
Traceback (most recent call last):
  File "/home/apetrella/Workspace/parallel-dl/FSDP/T5_training.py", line 225, in <module>
    fsdp_main(args)
  File "/home/apetrella/Workspace/parallel-dl/FSDP/T5_training.py", line 100, in fsdp_main
    train_dataset = wikihow(tokenizer, 'train', 1500, 512, 150, False)
  File "/home/apetrella/Workspace/parallel-dl/FSDP/summarization_dataset.py", line 28, in __init__
    self.dataset =  load_dataset('wikihow', 'all', data_dir='data/', split=type_path)
  File "/home/apetrella/miniconda3/envs/Parallel-DL/lib/python3.9/site-packages/datasets/load.py", line 2129, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/apetrella/miniconda3/envs/Parallel-DL/lib/python3.9/site-packages/datasets/load.py", line 1849, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/apetrella/miniconda3/envs/Parallel-DL/lib/python3.9/site-packages/datasets/load.py", line 1719, in dataset_module_factory
    raise e1 from None
  File "/home/apetrella/miniconda3/envs/Parallel-DL/lib/python3.9/site-packages/datasets/load.py", line 1645, in dataset_module_factory
    raise DatasetNotFoundError(f"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.") from e
datasets.exceptions.DatasetNotFoundError: Dataset 'wikihow' doesn't exist on the Hub or cannot be accessed.
W0220 14:05:31.983663 1251304 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1251320 closing signal SIGTERM
E0220 14:05:32.047662 1251304 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 1251319) of binary: /home/apetrella/miniconda3/envs/Parallel-DL/bin/python
Traceback (most recent call last):
  File "/home/apetrella/miniconda3/envs/Parallel-DL/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/apetrella/miniconda3/envs/Parallel-DL/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/apetrella/miniconda3/envs/Parallel-DL/lib/python3.9/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/home/apetrella/miniconda3/envs/Parallel-DL/lib/python3.9/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/home/apetrella/miniconda3/envs/Parallel-DL/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/apetrella/miniconda3/envs/Parallel-DL/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
T5_training.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-20_14:05:31
  host      : gn02.e4.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1251319)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
