W0319 14:44:55.166304 319594 site-packages/torch/distributed/run.py:792] 
W0319 14:44:55.166304 319594 site-packages/torch/distributed/run.py:792] *****************************************
W0319 14:44:55.166304 319594 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0319 14:44:55.166304 319594 site-packages/torch/distributed/run.py:792] *****************************************
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Saving the dataset (0/1 shards):   0%|          | 0/104743 [00:00<?, ? examples/s]Saving the dataset (0/1 shards):   0%|          | 0/104743 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|██████████| 104743/104743 [00:00<00:00, 317161.82 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 104743/104743 [00:00<00:00, 316964.80 examples/s]
Saving the dataset (1/1 shards): 100%|██████████| 104743/104743 [00:00<00:00, 318984.75 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 104743/104743 [00:00<00:00, 318788.01 examples/s]
Saving the dataset (0/1 shards):   0%|          | 0/5463 [00:00<?, ? examples/s]Saving the dataset (0/1 shards):   0%|          | 0/5463 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|██████████| 5463/5463 [00:00<00:00, 250922.42 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 5463/5463 [00:00<00:00, 249909.83 examples/s]
Saving the dataset (1/1 shards): 100%|██████████| 5463/5463 [00:00<00:00, 192174.00 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 5463/5463 [00:00<00:00, 191469.04 examples/s]
Saving the dataset (0/1 shards):   0%|          | 0/5463 [00:00<?, ? examples/s]Saving the dataset (0/1 shards):   0%|          | 0/5463 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|██████████| 5463/5463 [00:00<00:00, 253605.19 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 5463/5463 [00:00<00:00, 252607.08 examples/s]
Saving the dataset (1/1 shards): 100%|██████████| 5463/5463 [00:00<00:00, 151087.54 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 5463/5463 [00:00<00:00, 150531.69 examples/s]
/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/apetrella/miniconda3/envs/llm/lib/python3.9/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 530966 ON gn01 CANCELLED AT 2025-03-19T14:47:22 ***
slurmstepd: error: *** STEP 530966.0 ON gn01 CANCELLED AT 2025-03-19T14:47:22 ***
W0319 14:47:22.026933 319594 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received Signals.SIGTERM death signal, shutting down workers
W0319 14:47:22.027814 319594 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 319603 closing signal SIGTERM
W0319 14:47:22.028143 319594 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 319604 closing signal SIGTERM
